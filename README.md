# üß† Regra de Aprendizado Hebbiano e Suas Aplica√ß√µes em Machine Learning

A **Regra de Aprendizado Hebbiano** √© um princ√≠pio fundamental no campo do **Machine Learning** e redes neurais. Baseada no trabalho do neurocientista Donald Hebb, a regra de aprendizado estabelece que **as conex√µes entre os neur√¥nios s√£o ajustadas com base na sua atividade conjunta**, ou seja, **"neur√¥nios que disparam juntos, se conectam".** Esse conceito foi inicialmente desenvolvido para entender como os neur√¥nios do c√©rebro podem se ajustar e aprender com o tempo, mas se mostrou extremamente √∫til para a constru√ß√£o de modelos de aprendizado de m√°quina.

Neste reposit√≥rio, implementamos a **Regra de Hebb** para treinar um modelo simples de rede neural que aprende a partir das entradas e sa√≠das de fun√ß√µes l√≥gicas. Utilizando duas vari√°veis de entrada e um conjunto de fun√ß√µes l√≥gicas, o modelo ajusta os pesos da rede neural de acordo com os princ√≠pios da Regra de Hebb.

## üìö O que √© a Regra de Hebb?

A Regra de Hebb √© uma regra de aprendizado **n√£o supervisionado** em redes neurais, ou seja, o modelo ajusta os pesos das conex√µes entre os neur√¥nios com base nas entradas e sa√≠das observadas, mas sem precisar de r√≥tulos expl√≠citos.

A f√≥rmula b√°sica da Regra de Hebb √©:

$$
w(t+1) = w(t) + \eta \cdot y[i] \cdot x[i]
$$

Onde:
- \( w(t) \) √© o peso atual,
- \( w(t+1) \) √© o peso ajustado,
- \( \eta \) √© a taxa de aprendizado (normalmente um valor pequeno),
- \( y[i] \) √© a sa√≠da desejada para a entrada \( i \),
- \( x[i] \) √© o valor da entrada \( i \).
A cada itera√ß√£o, os pesos s√£o ajustados de acordo com a entrada e a sa√≠da, permitindo que a rede aprenda e ajuste as conex√µes de forma incremental.

## ‚öôÔ∏è A Import√¢ncia da Regra de Hebb no Machine Learning

A Regra de Hebb √© uma das primeiras tentativas de simular o comportamento de aprendizado do c√©rebro humano. Embora simples, ela possui uma grande relev√¢ncia para o desenvolvimento de redes neurais e modelos de **aprendizado n√£o supervisionado**. Ela est√° na base de muitos algoritmos de redes neurais e ajuda a entender o comportamento de como os neur√¥nios ajustam suas conex√µes.

### Vantagens da Regra de Hebb:
- **Aprendizado Local:** A regra √© baseada na intera√ß√£o local entre os neur√¥nios, o que permite que os sistemas de aprendizado operem sem a necessidade de supervis√£o externa.
- **Simplicidade:** Sua implementa√ß√£o √© simples e eficiente, sendo adequada para a constru√ß√£o de redes neurais iniciais e para o aprendizado de fun√ß√µes simples.
- **Fundamenta√ß√£o Biol√≥gica:** Como mencionado, a Regra de Hebb se inspira diretamente em como os neur√¥nios do c√©rebro humano aprendem e se conectam.

## üî¢ As Fun√ß√µes L√≥gicas e Suas Limita√ß√µes

Ao aplicar a Regra de Hebb a fun√ß√µes l√≥gicas, √© importante entender o conceito de **fun√ß√µes l√≥gicas com duas vari√°veis de entrada**. Existem 16 poss√≠veis combina√ß√µes para as entradas bin√°rias (0 e 1) para duas vari√°veis. No entanto, **nem todas essas 16 fun√ß√µes podem ser representadas com a Regra de Hebb**.

### Por que apenas 14 das 16 fun√ß√µes s√£o represent√°veis?

A raz√£o para isso est√° nas **propriedades matem√°ticas** da Regra de Hebb e como ela ajusta os pesos. A Regra de Hebb √© baseada na ideia de **fortalecer as conex√µes entre os neur√¥nios que s√£o ativados ao mesmo tempo**. No entanto, para que a rede aprenda corretamente todas as 16 fun√ß√µes l√≥gicas poss√≠veis, √© necess√°rio que as entradas e sa√≠das apresentem uma rela√ß√£o coerente para que a rede possa ajustar seus pesos de forma eficaz. 

**Das 16 fun√ß√µes l√≥gicas poss√≠veis com duas vari√°veis de entrada**, algumas n√£o podem ser representadas pela Regra de Hebb devido a **incompatibilidades matem√°ticas**. As fun√ß√µes que n√£o podem ser representadas s√£o aquelas que n√£o seguem uma rela√ß√£o linear entre as entradas e as sa√≠das, como:

- **Fun√ß√£o XOR (exclusive OR)**: Esta fun√ß√£o n√£o pode ser representada de forma simples porque a rela√ß√£o entre as entradas e a sa√≠da n√£o √© linear.
- **Fun√ß√£o XNOR**: Semelhante √† XOR, n√£o segue a mesma l√≥gica de aprendizado, tornando-a imposs√≠vel de aprender com a Regra de Hebb.

Por isso, apenas **14 das 16 fun√ß√µes l√≥gicas** podem ser representadas de forma eficaz. Essas fun√ß√µes s√£o aquelas que possuem uma rela√ß√£o linear ou que podem ser aprendidas sem a necessidade de intera√ß√£o mais complexa entre as entradas.

## üîç Fun√ß√µes L√≥gicas Implementadas

No c√≥digo, o modelo treina para aprender as seguintes **14 fun√ß√µes l√≥gicas** com duas vari√°veis de entrada:

1. **F1 (Constante 0):** Sa√≠da constante 0.
2. **F2 (Constante 1):** Sa√≠da constante 1.
3. **F3 (x1):** Sa√≠da igual ao valor de x1.
4. **F4 (x2):** Sa√≠da igual ao valor de x2.
5. **F5 (¬¨x1):** Nega√ß√£o de x1.
6. **F6 (¬¨x2):** Nega√ß√£o de x2.
7. **F7 (x1 ‚à® x2):** Opera√ß√£o l√≥gica OR.
8. **F8 (x1 ‚àß x2):** Opera√ß√£o l√≥gica AND.
9. **F9 (x1 ‚àß ¬¨x2):** Opera√ß√£o AND com a nega√ß√£o de x2.
10. **F10 (¬¨x1 ‚àß x2):** Opera√ß√£o AND com a nega√ß√£o de x1.
11. **F11 (x1 ‚äï x2):** Opera√ß√£o XOR (ou exclusivo).
12. **F12 (x1 ‚äô x2):** Opera√ß√£o NAND.
13. **F13 (¬¨(x1 ‚äô x2)):** Nega√ß√£o de x1 NAND x2.
14. **F14 (¬¨(x1 ‚äï x2)):** Nega√ß√£o de x1 XOR x2.

Essas fun√ß√µes s√£o usadas para demonstrar como a Regra de Hebb ajusta os pesos de forma incremental durante o treinamento, permitindo que a rede aprenda a partir de exemplos e generalize a partir deles.

## üìù Conclus√£o

A Regra de Hebb √© uma base importante no desenvolvimento de redes neurais, especialmente para aprendizado n√£o supervisionado. A aplica√ß√£o dessa regra em fun√ß√µes l√≥gicas simples √© uma excelente maneira de entender como os neur√¥nios ajustam suas conex√µes e como a aprendizagem pode ocorrer de maneira simples e eficiente. A limita√ß√£o de representabilidade de algumas fun√ß√µes l√≥gicas √© um aspecto interessante que nos ajuda a compreender melhor as condi√ß√µes necess√°rias para o aprendizado com a Regra de Hebb.

Ao aprender essas fun√ß√µes l√≥gicas, o modelo pode ser estendido para problemas mais complexos de aprendizado de m√°quina e redes neurais, pavimentando o caminho para sistemas mais avan√ßados.